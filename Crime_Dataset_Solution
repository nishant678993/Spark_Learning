Exercise 01 - Get monthly crime count by type
Dataset - https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2/data

Solution 1-
Using Dataframe

crimes_df = spark.read.load("/FileStore/tables/Crimes___2001_to_present-9be1b.csv",format="csv",header="true")
from pyspark.sql.functions import *
crimes_df2 = crimes_df.select("id","Primary Type",split(col("Date"),"/").getItem(0).alias("Month"),split(col("Date"),"/").getItem(1).alias("Day"),split(split(col("Date"),"/").getItem(2).alias("Yearwithtime")," ").getItem(0).alias("Year"))
crimes_df2.registerTempTable("crimes_rdd")
spark.catalog.cacheTable("crimes_rdd")
crimes_df3 = spark.sql("SELECT cast(concat(Year,Month)AS STRING)AS YearMonth, `Primary Type` AS `crimetype`,cast(count(`Primary Type`)AS STRING) AS `crimecount` from crimes_rdd group by Month ,Year ,`Primary Type` order by Month asc , Year asc ,count(`Primary Type`) desc")
crimes_df3.show()
crimes_df3.write.csv(path="/FileStore/tables/crimes_by_type_by_month",mode="overwrite",sep=",", compression="gzip",header=True)




Solution 2 -
Using RDD and converting to Dataframe-


from pyspark.sql import Row
from pyspark.sql.functions import *
crimesRDD_1 = sc.textFile("/FileStore/tables/Crimes___2001_to_present-9be1b.csv")
crimesRDD_2= crimesRDD_1.map(lambda r : ((r.split(",")[2],r.split(",")[5])))
header = crimesRDD_2.first()
crimesRDD_3 = crimesRDD_2.filter(lambda s: s!=header)
crimes_df = crimesRDD_3.map(lambda r: Row(Month = r[0].split("/")[0],Year = r[0].split("/")[2].split(" ")[0],Primary_Type= r[1])).toDF()
crimes_df.registerTempTable("crimesRDD_to_df")
spark.catalog.cacheTable("crimesRDD_to_df")
crimes_df_file = sqlContext.sql("SELECT concat(Year,Month)AS Month, Primary_Type AS crimetype,count(Primary_Type) AS crimecount from crimesRDD_to_df group by Month ,Year ,Primary_Type order by Month asc , Year asc ,count(Primary_Type) desc")
crimes_df_file.show()
crimes_df_file.write.csv(path="/FileStore/tables/crimes_by_type_by_month_rdd_to_df",mode="overwrite",sep=",", compression="gzip",header=True)

